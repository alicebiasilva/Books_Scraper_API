# API P√∫blica para Consulta de Livros 

---

API robusta e eficiente para coleta, armazenamento e consulta de dados de livros do site "Books to Scrape" (https://books.toscrape.com/), projetada para garantir alta disponibilidade e seguran√ßa, potencializando seus projetos de ci√™ncia de dados com informa√ß√µes precisas e atualizadas. 

Autora: Alice Beatriz da Silva

√öltima atualiza√ß√£o: 12/10/2025

Dispon√≠vel em: https://pos-tech-machine-learning-engineer-eta.vercel.app/

---

## √çndice

- [Funcionalidades](#funcionalidades)  
- [Tecnologias](#tecnologias)  
- [Estrutura do projeto](#estrutura-do-projeto)  
- [Instala√ß√£o e Configura√ß√£o](#instalacao-e-configuracao)  
- [Escalabilidade](#escalabilidade)

---

## Funcionalidades

### Por que este projeto √© ideal para cientistas de dados?

Este projeto facilita o trabalho repetitivo e burocr√°tico da coleta e organiza√ß√£o dos dados, permitindo que voc√™ foque no que realmente importa: gerar insights e construir modelos preditivos de alta qualidade.

- **Automatiza a coleta de dados**: raspagem eficiente e estruturada do site "Books to Scrape" (https://books.toscrape.com/), eliminando o trabalho manual e economizando tempo.
- **Armazenamento organizado**: dados salvos em formato CSV padr√£o, facilitando integra√ß√£o com ferramentas de an√°lise e modelagem.
- **Consulta r√°pida e flex√≠vel**: API REST permite filtrar livros por t√≠tulo, categoria, ou ambos, agilizando a extra√ß√£o dos dados relevantes para suas an√°lises.
- **Imagens acess√≠veis**: capas dos livros s√£o baixadas e disponibilizadas via URL p√∫blica, enriquecendo dashboards e relat√≥rios visuais.
- **Design escal√°vel e seguro**: pensado para suportar aumento de volume e garantir a integridade dos dados, essencial para projetos que crescem com o tempo.
- **F√°cil integra√ß√£o**: pode ser incorporado facilmente em pipelines de ci√™ncia de dados, machine learning e visualiza√ß√£o, reduzindo barreiras t√©cnicas.

### Endpoints da API

| M√©todo | Rota               | Descri√ß√£o                                                              |
| ------ | ------------------ | ---------------------------------------------------------------------- |
| `GET`  | `/books`           | Retorna uma lista com os t√≠tulos de todos os livros.                   |
| `GET`  | `/books/{book_id}` | Retorna os detalhes completos de um livro com base no seu ID.          |
| `GET`  | `/books/search`    | Permite buscar livros por t√≠tulo, categoria ou ambos.                  |
| `GET`  | `/categories`      | Lista todas as categorias √∫nicas dispon√≠veis.                          |
| `GET`  | `/health`          | Verifica o status da API e da base de dados (√∫til para monitoramento). |


### Exemplos de uso

### `GET /books`: 

Lista todos os t√≠tulos dispon√≠veis.

  ```
  https://pos-tech-machine-learning-engineer.vercel.app/api/v1/books
  ```


#### Resposta

```json
[
  "It's Only the Himalayas",
  "Tipping the Velvet",
  "Soumission",
  "Sharp Objects",
  "Sapiens",
  ...
]
```

### `GET /books/{book_id}`: 

Busca todas as informa√ß√µes de um livro por ID (exemplo ID = 1).

  ```
  https://pos-tech-machine-learning-engineer.vercel.app/api/v1/books/1
  ```

#### Resposta

```json
{
  "id": 1,
  "title": "It's Only the Himalayas",
  "price": 45.17,
  "availability": "In stock",
  "rating": "Two",
  "category": "Travel",
  "image_path": "https://alicebiasilva.github.io/Pos_Tech_Machine_Learning_Engineer/public/images/27a53d0bb95bdd88288eaf66c9230d7e.jpg"
}
```

### `GET /books/categories`: 

Lista todas as categorias dispon√≠veis.
```
  https://pos-tech-machine-learning-engineer.vercel.app/api/v1/categories
  ```
#### Resposta

```json
{
  "categories": [
    "Travel",
    "Mystery",
    "Historical Fiction",
    "Science Fiction",
    "Poetry",
    ...
  ]
}
```

### `GET /books/search`: 

Buscar por t√≠tulo e/ou categoria juntos:

  ```
  https://pos-tech-machine-learning-engineer.vercel.app/api/v1/books/search?category=Travel
  ```

* `GET /health`:Valida sa√∫de da API, ou seja, se as informa√ß√µes est√£o dispon√≠veis:
```
  https://pos-tech-machine-learning-engineer.vercel.app/api/v1/health
  ```
### Resposta

```json
{
  "status": "ok"
}
```

---

## Tecnologias

### Bibliotecas e Ferramentas de Desenvolvimento
- **FastAPI** ‚Äì Framework para construir APIs r√°pidas e escal√°veis.
- **Pandas** ‚Äì Manipula√ß√£o e an√°lise de dados.
- **BeautifulSoup** ‚Äì Raspagem e parsing de HTML.
- **Requests** ‚Äì Requisi√ß√µes HTTP simples e elegantes.
- **Black** ‚Äì Formatador de c√≥digo Python para manter a consist√™ncia e qualidade do c√≥digo.

### Tecnologias de Deploy e Versionamento
- **Vercel** ‚Äì Plataforma para deploy e hospedagem r√°pida da API em produ√ß√£o.
- **GitHub** ‚Äì Controle de vers√£o e hospedagem do c√≥digo-fonte.

### Destaques do Projeto

- **C√≥digo Modularizado:** O projeto √© estruturado em m√≥dulos claros e organizados, facilitando manuten√ß√£o e escalabilidade.
- **C√≥digo Limpo e Padronizado:** Uso do Black para garantir consist√™ncia e legibilidade do c√≥digo.
- **Documenta√ß√£o Completa:** Todas as fun√ß√µes e classes possuem docstrings detalhadas, e o c√≥digo est√° amplamente comentado para facilitar o entendimento.
- **API Bem Estruturada:** Endpoints claros e intuitivos para consulta eficiente dos dados coletados.
- **Frontend acess√≠vel** ‚Äì Desenvolvido para facilitar o acesso e a intera√ß√£o com a API, proporcionando uma experi√™ncia amig√°vel para os usu√°rios.
- **Pronto para Produ√ß√£o:** Deploy simplificado via Vercel, garantindo alta disponibilidade e performance.

---
 
## üíª Estrutura do projeto {#estrutura-do-projeto}

O projeto est√° organizado de forma modular, com os arquivos e pastas logicamente separados para facilitar manuten√ß√£o, escalabilidade e reaproveitamento do c√≥digo. Essa organiza√ß√£o ajuda a manter o c√≥digo limpo e claro, al√©m de permitir que cada componente seja desenvolvido e testado isoladamente.

* A pasta `scrapping` cont√©m as fun√ß√µes respons√°veis por extrair os dados (`extract_data.py`) e carregar/salvar esses dados (`load_data.py`). Separar essas responsabilidades em arquivos distintos torna o projeto mais escal√°vel, pois essas fun√ß√µes podem ser reaproveitadas ou expandidas futuramente sem impactar outras partes do sistema.
* A pasta `api` √© dedicada √† defini√ß√£o das rotas da API e ao armazenamento dos dados coletados, mantendo a l√≥gica da aplica√ß√£o desacoplada do processo de coleta.
* A pasta `public` guarda os recursos est√°ticos, como as imagens baixadas, garantindo que estejam acess√≠veis para a API e poss√≠veis frontends.
* Arquivos como `main.py`, `requirements.txt` e `vercel.json` cuidam da inicializa√ß√£o da aplica√ß√£o, das depend√™ncias e da configura√ß√£o do deploy.

Essa separa√ß√£o clara e modular facilita o desenvolvimento colaborativo, o teste, a manuten√ß√£o e a expans√£o futura do projeto.


| Caminho                          | Descri√ß√£o                                                                 |
|----------------------------------|---------------------------------------------------------------------------|
| `api/data/books.csv`            | Arquivo CSV contendo os dados coletados dos livros                       |
| `api/routes.py`                 | Defini√ß√£o das rotas da API para consulta dos dados                       |
| `api/config.py`                 | Configura√ß√µes gerais da aplica√ß√£o (ex: chave secreta, cache)             |
| `api/__init__.py`              | Torna a pasta `api` um pacote Python                                     |
| `public/images/`               | Diret√≥rio onde as imagens dos livros s√£o armazenadas ap√≥s o download     |
| `scrapping/extract_data.py`    | Fun√ß√µes respons√°veis pelo scraping dos dados do site                     |
| `scrapping/load_data.py`       | Fun√ß√µes para salvar os dados coletados em CSV                            |
| `scrapping/scrape_and_save.py` | Script que integra scraping e armazenamento dos dados                    |
| `scrapping/__init__.py`        | Torna a pasta `scrapping` um pacote Python                               |
| `main.py`                      | Script principal que inicia o scraping e roda a API                      |
| `requirements.txt`             | Lista das depend√™ncias Python necess√°rias para o projeto                 |
| `vercel.json`                  | Configura√ß√µes para deploy no Vercel                                      |
| `.gitignore`                   | Arquivos e pastas ignorados pelo Git                                     |
| `README.md`                    | Documenta√ß√£o do projeto                                                  |



## Instala√ß√£o e Configura√ß√£o

Siga os passos abaixo para clonar, configurar e rodar o projeto localmente:

#### 1. Clone o reposit√≥rio

```bash
git clone https://github.com/seu-usuario/seu-repo.git
cd seu-repo
```

#### 2. Crie um ambiente virtual (opcional, mas recomendado)

```bash
python -m venv venv
source venv/bin/activate  # Linux/macOS
venv\Scripts\activate     # Windows
```

#### 3. Instale as depend√™ncias

```bash
pip install -r requirements.txt
```

#### 4. Rode o scraper para coletar os dados

```bash
python scrapping/scrape_and_save.py
```

Esse comando ir√°:

* Fazer o scraping de todos os livros do site [Books to Scrape](https://books.toscrape.com/)
* Baixar as imagens dos livros
* Salvar os dados estruturados no arquivo `api/data/books.csv`

#### 5. Execute a API localmente

```bash
uvicorn main:app --reload
```

A API estar√° dispon√≠vel em: [http://127.0.0.1:8000](http://127.0.0.1:8000)

#### 6. Acesse a documenta√ß√£o interativa (Swagger)

Ap√≥s rodar o servidor, voc√™ pode visualizar e testar todas as rotas acessando:

```
http://127.0.0.1:8000/docs
```

---

## Escalabilidade

### Pr√≥ximos passos

Entendemos que a constru√ß√£o de produtos robustos deve ser feita de forma cont√≠nua, colaborativa e centrada no usu√°rio. Por isso, esta √© a **primeira vers√£o** do projeto ‚Äî totalmente funcional, mas com espa√ßo para melhorias e evolu√ß√£o.

Estamos abertos a feedbacks e j√° mapeamos algumas melhorias para as pr√≥ximas entregas:

#### Automatiza√ß√£o e Atualiza√ß√£o

* **Definir frequ√™ncia de atualiza√ß√£o dos dados**, por exemplo via agendamento com `cron`, GitHub Actions ou cloud functions (AWS Lambda, Google Cloud Functions).
* **Agendar o scraping periodicamente**, garantindo que os dados estejam sempre atualizados.

#### Containeriza√ß√£o com Docker

* Criar um `Dockerfile` e um `docker-compose.yml` para facilitar o deploy local e em produ√ß√£o.
* Padronizar ambientes para desenvolvedores e garantir portabilidade entre m√°quinas.

#### Banco de Dados em Nuvem

* Integrar a persist√™ncia dos dados com um banco de dados relacional (PostgreSQL, MySQL) ou NoSQL (MongoDB). Isso permitir√° maior escalabilidade, consultas mais complexas e integra√ß√£o com dashboards.

#### Pain√©is e Visualiza√ß√£o

* Criar um dashboard interativo (ex: Streamlit, Dash ou Power BI conectado via API) para visualiza√ß√£o r√°pida dos livros, categorias e pre√ßos.
* Integrar com ferramentas de visual analytics para insights mais profundos.

#### Testes Automatizados

* Criar testes unit√°rios e de integra√ß√£o com `pytest`.
* Configurar CI/CD no GitHub para rodar os testes automaticamente a cada push.

#### Versionamento Sem√¢ntico

* Adotar versionamento sem√¢ntico (`semver`) para releases (`v1.0.0`, `v1.1.0` etc).
* Manter changelog atualizado no GitHub.
